{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "bqVZBxrvPD8n",
        "mDgbUHAGgjLW",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "aJV4KIxSnxay",
        "u3PMJOP6ngxN",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "xJivPyE8q_2k",
        "azX1PEddq_20",
        "iyKleWeyq_20",
        "8-UX51ofq_21",
        "Of3PJYNbrGff",
        "w8bcvZxarGfg",
        "HZ6txCBBrGfh",
        "jbkhcZqdrGfh",
        "riLp7y9brHca",
        "Uw0GT_uHrHcb",
        "qTEiGUKorHcc",
        "1Qw8mtL3rHcd",
        "xnMZvg2orIPt",
        "mwHQVoIQrIPu",
        "Kwy5hImkrIPu",
        "0sjxqkK0rIPv",
        "y7avNpR9HL5z",
        "Z5yPuW4iHL50",
        "3vIdo55JHL50",
        "IlhlbgNBHL51",
        "yxjklrqffwwK",
        "t2v8hxNAfwwL",
        "YveDNi1QfwwM",
        "oQ-pHCWMHO92",
        "DEMFeE18HO92",
        "MR3eS7xTHO93",
        "JmI1Y4YbHO93",
        "XGShqbS2HSAY",
        "B4XxyvTzHSAZ",
        "Mal1wQlpHSAZ",
        "BgP_e3WlHSAZ",
        "I3aNiIh9fKqZ",
        "hTF1e9PwfKqa",
        "1xrBLU6IfKqa",
        "yiLHMWFSfKqb",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "jLrtcv83OEmf",
        "5djnGilGOEmg",
        "DADo_qtwOEmg",
        "0VL2QIwzOEmh",
        "69qjvWOxOEmh",
        "FqZJ_xAgOH3o",
        "pE65w-N1OH3p",
        "OISwWHvhOH3p",
        "rheC10M4OH3q",
        "-CtoKDX7OH3q",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "peAK6Cc_HQeo",
        "khncImPpHcol",
        "wTe8K5rdYGV1",
        "yyzM232tatvC",
        "OMC09DFCbrEm",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "eLTSyMe3_MXH",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1991Ab/Capstone-project-2-Taxi-trip-time-Prediction/blob/main/Taxi_trip_prediction_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= input()\n",
        "b= input()\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "PCIR1YG8oP9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  NYC Taxi Trip Time Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "ILM16xV1PD8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "bqVZBxrvPD8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BUSINESS PROBLEM OVERVIEW**\n",
        "\n",
        "\n",
        "Your task is to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables."
      ],
      "metadata": {
        "id": "jivUA-ONOwse"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-bbhD0YpUgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "from numpy import loadtxt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib import rcParams\n",
        "!pip install pymysql\n",
        "import pymysql\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.pool import NullPool\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy.stats import *\n",
        "import math\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import Ridge, RidgeCV\n",
        "from sklearn.linear_model import Lasso, LassoCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import XGBRFRegressor\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "!pip install shap==0.40.0\n",
        "import shap \n",
        "import graphviz\n",
        "sns.set_style('darkgrid') \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Mounting the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing the dataset\n",
        "file_path = \"/content/drive/MyDrive/Capstone project-2/NYC Taxi Data (1).csv\"\n",
        "df = pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "id": "CEhiayUv1809"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Fu3GGMbrjRut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First \n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns \n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the NYC taxi trip time prediction data set contains 1458644 rows and 11 columns"
      ],
      "metadata": {
        "id": "2_V23HLH08TZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see the details of each column present in the dataset such as  their data type and count of values "
      ],
      "metadata": {
        "id": "9pA4lAiE1Y0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is very essential to check if our data set has any duplicate values and remove them if any.We can find out the duplicate values using the function dataset.duplicated().  len(dataset[dataset.duplicated()]) gives the count of duplicate values.From the above code we can see that there are no duplicate values in the dataset."
      ],
      "metadata": {
        "id": "MC6iLaZH3tE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is very important to check for missing values or NAN values as it has a very high impact on the model.Hence identifying the NAN values and treating them becomes very important.From the above data and graph we can see that the dataset has no missing or NAN values."
      ],
      "metadata": {
        "id": "LWEeYISd5d5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset given is based on the  NYC Yellow Cab trip record data.We have to predict the taxi trip duration of various rides using various Machine Learning models.The data set has 'trip_duration' as a continous target variable and many independent variables like 'passenger_count' , 'pickup_datetime' , 'dropoff_datetime' etc.The  dataset has 1458644 rows and 11 columns.It has no NAN values and duplicate values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "n87BaXA_42-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are the variables used in dataset"
      ],
      "metadata": {
        "id": "z4ksAAsB-fCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### id - a unique identifier for each trip\n",
        "* #### vendor_id - a code indicating the provider associated with the trip record\n",
        "* #### pickup_datetime - date and time when the meter was engaged\n",
        "* #### dropoff_datetime - date and time when the meter was disengaged\n",
        "* #### passenger_count - the number of passengers in the vehicle (driver entered value)\n",
        "* #### pickup_longitude - the longitude where the meter was engaged\n",
        "* #### pickup_latitude - the latitude where the meter was engaged\n",
        "* #### dropoff_longitude - the longitude where the meter was disengaged\n",
        "* #### dropoff_latitude - the latitude where the meter was disengaged\n",
        "* #### store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n",
        "* #### trip_duration - duration of the trip in seconds"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")\n",
        "#df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Create a copy of the current dataset and assigning to df\n",
        "new_data=df.copy()\n",
        "# Checking Shape of True Value\n",
        "new_data.shape\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the distance between pickup and dropoff locations using haversine formula"
      ],
      "metadata": {
        "id": "PaPWuHPHbnEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip  install haversine\n",
        "from numpy import math\n",
        "from haversine import haversine\n",
        "def trip_distance(df) :\n",
        "  trip_pickup=(df['pickup_latitude'], df['pickup_longitude'])\n",
        "  trip_dropoff=(df['dropoff_latitude'], df['dropoff_longitude'])\n",
        "  return haversine(trip_pickup , trip_dropoff)"
      ],
      "metadata": {
        "id": "NrK899PLgN2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the distance and creating a new column \"distance\" in the dataset\n",
        "df['distance']=df.apply(lambda x:trip_distance(x) , axis=1)"
      ],
      "metadata": {
        "id": "-d-_JWvIbpZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['distance'].reset_index\n"
      ],
      "metadata": {
        "id": "4xoF-yzBbpb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "t9qYRB4PbpWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the customer with highest trip distance\n"
      ],
      "metadata": {
        "id": "uu3RSwCGyTvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_distance=df.groupby('id')['distance'].max().reset_index().sort_values(by='distance' , ascending=False)"
      ],
      "metadata": {
        "id": "c7oc-9h4bpUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_distance"
      ],
      "metadata": {
        "id": "gxvSzSDJz1aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graphical Representation\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['figure.figsize'] = (14,5)\n",
        "sns.barplot(x='id' , y='distance' ,  data=max_distance.head(10))"
      ],
      "metadata": {
        "id": "6-4SizNz0MSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above plot that customer id'id2306955' has the highest distance for trip "
      ],
      "metadata": {
        "id": "teEwHeda1ceY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHw3BLGOKvC3"
      },
      "source": [
        "Now let us create four new features from 'pickup_datetime' and 'dropoff_datetime' for better analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "hFIpRvSJ5ZRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the data type of variables 'pickup_datetime' and 'dropoff_datetime' to datetime\n",
        "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
        "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])"
      ],
      "metadata": {
        "id": "bamNgSRn5KcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxezIBW-OsfT"
      },
      "outputs": [],
      "source": [
        "df[\"year\"] = df[\"pickup_datetime\"].apply(lambda x: x.year)\n",
        "df[\"pickup_month\"] = df[\"pickup_datetime\"].apply(lambda x: x.month)\n",
        "df[\"dropoff_month\"] = df[\"dropoff_datetime\"].apply(lambda x: x.month)\n",
        "df[\"pickup_day\"] = df[\"pickup_datetime\"].apply(lambda x: x.weekday())\n",
        "df[\"dropoff_day\"] = df[\"dropoff_datetime\"].apply(lambda x: x.weekday())\n",
        "df[\"dropoff_time_hour\"] = df[\"dropoff_datetime\"].apply(lambda x: x.hour)\n",
        "df[\"dropoff_time_min\"] = df[\"dropoff_datetime\"].apply(lambda x: x.minute)\n",
        "df[\"pick_up_time_hour\"] = df[\"pickup_datetime\"].apply(lambda x: x.hour)\n",
        "df[\"pick_up_time_min\"] = df[\"pickup_datetime\"].apply(lambda x: x.minute)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now,let us analyse the trip duration for a  week"
      ],
      "metadata": {
        "id": "eVLx0s8-HCQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duration=df.groupby(\"pickup_day\")['trip_duration'].mean().reset_index().sort_values(by='trip_duration' , ascending=False)\n",
        "duration"
      ],
      "metadata": {
        "id": "O0oXNLKoKMY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graphical Representation\n",
        "sns.pointplot(x='pickup_day' , y='trip_duration' , data=duration)\n",
        "plt.ylabel('Trip Duration')\n",
        "plt.xlabel('Weekday')\n",
        "plt.title('Trip Duration per WeekDay')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UQsZaT5ZJSmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we can infer that the duration of trip is maximum on Thursday represented as day-'3'."
      ],
      "metadata": {
        "id": "QqJOt95LLUo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us see the customer who have highest trip duration "
      ],
      "metadata": {
        "id": "AOx_SBBhxgZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_distance=df.groupby(\"id\")[\"trip_duration\"].mean().reset_index().sort_values(by=\"trip_duration\" , ascending=False)\n",
        "customer_distance\n",
        "\n"
      ],
      "metadata": {
        "id": "RzWtvo7wCmtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now,we will analyse all the vedor's trip duration and distance travelled for each month"
      ],
      "metadata": {
        "id": "qqlSxeSza9NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vendor_info=df.groupby([\"vendor_id\" , \"pickup_month\"] , as_index=False).agg({\"trip_duration\" : \"mean\" , \"distance\" : \"mean\"}).reset_index()\n",
        "vendor_info"
      ],
      "metadata": {
        "id": "z9jnIRJKaqI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding out the vendor with highest trip duration\n",
        "vendor_info.sort_values(\"trip_duration\" , ascending=False).head(3)"
      ],
      "metadata": {
        "id": "0sSYSqGLbafC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis we that it is 'Vendor 2' covering a highest duration of an average 1129.24 seconds and it is in the month of June represented by'6'.The second  and the third place is also occupied by 'Vendor 2'.The second highest trip covered was in the month of May represented by 5 and the third highest trip was covered in the month of 'April' represented by 4 in the above analysis."
      ],
      "metadata": {
        "id": "tQHMyH0UcPs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will find out the number of customers who travel within 30 mins,customers who travel between 30 mins to 1hr and customers who travel from 1hr to 2hr and customers who travel above 2hrs and more."
      ],
      "metadata": {
        "id": "KxtOh66oDGcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividing the dataset on the basis of cutomer travelling time.\n",
        "#Determining the count of customers who travel within 30 mins\n",
        "df_travel_time_30=df[df[\"trip_duration\"]<=1800]\n",
        "df_travel_time_30.head()\n"
      ],
      "metadata": {
        "id": "pKq26__WDur7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_travel_time_30))"
      ],
      "metadata": {
        "id": "QwUdM2uVJZqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above data that the count of customer travelling within 30 mins is really very large which is around 1345526"
      ],
      "metadata": {
        "id": "RrTurcuuJfXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Determinig the count of customers who travel from 30 mins to 1hour.\n",
        "df_travel_time_60=df[(df[\"trip_duration\"]<=3600)& (df[\"trip_duration\"]>1800)]\n",
        "df_travel_time_60.head()"
      ],
      "metadata": {
        "id": "hBmapjSBJ1fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_travel_time_60))"
      ],
      "metadata": {
        "id": "4A80ImwtKThT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis we can see that the number of customers who travel \n",
        "between 30 mins to 1 hour is 100801.It is pretty less compared to number of customer who travel within 30 mins. "
      ],
      "metadata": {
        "id": "tsiVXpQdHXOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Determiming the count of customers who travel between 1 hours to 2 hours.\n",
        "df_travel_time_2hr=df[(df[\"trip_duration\"]>3600) & (df[\"trip_duration\"]<=(3600*2))]\n",
        "df_travel_time_2hr.head()"
      ],
      "metadata": {
        "id": "QmexhAbaLRrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_travel_time_2hr))"
      ],
      "metadata": {
        "id": "P8nmP4miMOQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the number of customers who travel between 1 hour to 2hour is 10064."
      ],
      "metadata": {
        "id": "ydtuRIiEMcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Determiming the count of customers who travel for more than 2 hours.\n",
        "df_travel_time=df[df[\"trip_duration\"]>(3600)*2]\n",
        "df_travel_time.head()"
      ],
      "metadata": {
        "id": "TPzTNFK4Mah-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_travel_time))"
      ],
      "metadata": {
        "id": "2DzGijUoMaqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis we can see that the number of customers having travel time of above 2 hours is 2253 which is very less compared to the number of customers travelling within 30 mins,1hrs and 2hrs."
      ],
      "metadata": {
        "id": "JuXm9uiiOU1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to a perception we will get a clear view of customers and their tavel \n",
        "duration by graphical representaions.However for analysing the customers trip duration it is necessary to deep dive into the data set to unearth many insights.So I have created a new column called \"Distance\" using the haversine formula.I have analysed the customers with highest  distance and highest trip_duration.\n",
        "I have broken down the \"pickup_datetime\" column,\"droppoff_datetime\" column into 8 columns which are:-\n",
        "\"pickup_month\" , \"pick_up_time_hr\" , \"pick_up_time_min\" , \"pick_up_day\",\n",
        "\"dropoff_month\" , \"dropoff_time_hr\" , \"dropoff_time_min\" , \"dropoff_day\"\n",
        "Following are the insights that were analysed:\n",
        "\n",
        "*   From the above analysis we see that it is 'Vendor 2' covering a highest  \n",
        "    duration of an average 1129.24 seconds and it is in the month of June \n",
        "    represented by'6'.The second  and the third place is also occupied by \n",
        "    'Vendor 2'.The second highest trip covered was in the month of May \n",
        "    represented by 5 and the third highest trip was covered in the month of \n",
        "    'April' represented by 4 in the above analysis.\n",
        "\n",
        "\n",
        "*   The highest duration was done by cutomers whose booking id was 'id0053347'.\n",
        "    It is found out that the trip duration for this trip is 979.52 hours.\n",
        "\n",
        "*   The highest distance was travelled by the customers whose booking id is\n",
        "    found to be 'id2306955'.The distance travelled is approximately  1240.91\n",
        "\n",
        "*   We can see from the  data analysis that the count of customer      \n",
        "    travelling within 30 mins is really very large which is around '1345526'   \n",
        "\n",
        "*   From the above analysis it is inferred that the number of customers who \n",
        "    travelbetween 30 mins to 1 hour is 100801.It is pretty less compared to   \n",
        "    number of customer who travel within 30 mins.\n",
        "\n",
        "*   We can see that the number of customers who travel between 1 hour to 2hour \n",
        "    is 10064.It has become less when compared to the number of customer \n",
        "    travelling within 30 mins and number of customer travelling between 30 mins\n",
        "    to 1 hour\n",
        "\n",
        "*   From the above analysis we can see that the number of customers having  \n",
        "    travel time of above 2 hours is 2253 which is very less compared to the  \n",
        "    number of customers travelling within 2hrs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l7Sf9RblRrPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Horizontal Bar plot on Dependant Variable i.e., Trip Duration (Univariate)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "sns.set_style('white')\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['figure.figsize'] = (14,5)"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.trip_duration.groupby(pd.cut(df.trip_duration, np.arange(0,9000,600))).count().plot(kind='barh')\n",
        "plt.xlabel('Trip Counts')\n",
        "plt.ylabel('Trip Duration in seconds ')\n",
        "plt.title(\"Count Of Trip Duration\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pgVpstuR6oNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar graph is used to compare data among categories.It also shows the  counts of values for the different levels of a categorical or nominal variable.To visualize the number of trip counts for various set of trip durations,I used bar chart."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot I got to know that the amount of customers who travel between 0-600 seconds is very large.A lot of people prefer to do short trips in taxi cab wherein the duration is within 10mins.Very few people are booking cabs for longer trips.The number of trips is decresing with the increase in trip duration.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a taxi cab owner It will be important to analyse the trips that are being booked.Wheather they are  longer trips where the trip duration is more tha an a hour or shorter trips where in the trip ends within 10 mins.This analysis can help to decide the fare of each trip and where to concentrate more number of cabs."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Countplot for Vendor_Id"
      ],
      "metadata": {
        "id": "xJivPyE8q_2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "sns.set_style('white')\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['figure.figsize'] = (5,5)\n",
        "sns.countplot(df[\"vendor_id\"], palette=\"husl\").set_title(\"Market Share Of Vendor Id\")"
      ],
      "metadata": {
        "id": "NTkxmIkWq_20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "azX1PEddq_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot Shows the counts of observations in each categorical bin using bars.I used count plot to visualise the categorical feature 'vendor_id' to analyse which vendor has more market share"
      ],
      "metadata": {
        "id": "QTXVnJ-fq_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "iyKleWeyq_20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot I got to know that there are two vendors who are providing cab services which are 'Vendor_id1' and 'Vendor_id2'.From the countplot it is evident that  'Vendor_id2' has more number of customers than Vendor_id1'"
      ],
      "metadata": {
        "id": "DwesNe8zq_21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "`[# This is formatted as code](https:// [link text](https://))`\n",
        "```\n",
        "\n",
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "8-UX51ofq_21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,as a cab owner it becomes important to analyse which all are vendors who are making a high profit and are more popular among the pepole.It will also be crucial to get to know the factors that are leading to more popularity among the people which inturn leads to a high market share  "
      ],
      "metadata": {
        "id": "nW7-bqD0q_21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Pie chart on \"Store_Forward\" flag"
      ],
      "metadata": {
        "id": "Of3PJYNbrGff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "df[\"store_and_fwd_flag\"].value_counts().plot(kind=\"pie\" , labels=['Yes' , 'No'] , autopct=\"%1.1f%%\" , shadow=True , colors=['skyblue' , 'red'] , explode=[0,0])"
      ],
      "metadata": {
        "id": "tnFltgWlrGfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "w8bcvZxarGfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in our. data.Each slice represents one component and all slices added together equal the whole.It becomes easy to visualize data with pie charts and each slice can be analysed easily.I used pie chart to anlyse \"store_and_forward\"  flag.\n"
      ],
      "metadata": {
        "id": "9-GBsMEqrGfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "HZ6txCBBrGfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above pie chart I came to know that around 99.4% of cabs record the trip record data before sending it to the vendor.Only around 0.6% of cabs have not recorded the cab data before sending it to the vendor.The trip might not hav been recorded due to either poor infrastructure provided by the vendors or due to signal loss that might have happend during  recording of the trip.However analysis shows that all most all cabs prefer to store the recoeded trip."
      ],
      "metadata": {
        "id": "sn1AvYFqDo7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "jbkhcZqdrGfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above analysis indicates how many of the cabs stotre the trip record before sending it to the vendor.Definetly its helps in creating a positive impact on business.For the taxi companies the trip record data is a very important and crucial data which has to be stored carefully.Storing these types of data help companies to understand the customer needs and can provide better customer satisfaction.Storing the trip record data in vehicle memomry before sending to the vendor means we are duplicating the data.If some mishap happens in the vendor's server end the data will not be destroyed.It can be easily retrived by the vehicle's memory.It sets as an example for good infrastructure and technology provided.Also It helps in providing good customer satisfaction."
      ],
      "metadata": {
        "id": "sZjMyjAHrGfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Taxi pick up time vs Passenger count(Bivariate)\n",
        "\n",
        "> \n",
        "\n"
      ],
      "metadata": {
        "id": "riLp7y9brHca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_number=df.groupby(\"pick_up_time_hour\")[\"passenger_count\"].count().reset_index().sort_values(by=\"passenger_count\" , ascending=False)\n",
        "passenger_number"
      ],
      "metadata": {
        "id": "tFsUD5VxVhbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('white')\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['figure.figsize'] = (8,8)\n",
        "sns.lineplot(data=passenger_number, x=\"pick_up_time_hour\", y=\"passenger_count\").set(title =\"The number of passengers travelling in cabs\")\n"
      ],
      "metadata": {
        "id": "VAwnqucEYHmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Uw0GT_uHrHcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart is used to visualize the change in one variable with the change in time or any other continous variable.I used Line plot to track and visualize the change in the number of passengers onbaording cabs with time."
      ],
      "metadata": {
        "id": "R7c_a-IXrHcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "qTEiGUKorHcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart I came to know that the number of people travelling is maximum at 6:00 PM in the evening.From early morning 1:00 AM till 5 AM the number of people travelling is very minimal.The number of people travelling increases linearly from 5 AM untill 10 AM.After 10 AM till 3 PM there is a very few increase in the number of passengers.It dips at 4.00PM however reaches its peak at 6.00 pm.After 6.00 PM in the evening the number of passengers travelling decreases significantly."
      ],
      "metadata": {
        "id": "hOCZkHS8iA9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "1Qw8mtL3rHcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,definetly the above analysis will have a positive impact on the business.\n",
        "As a company running taxi cabs , it becomes very important to analyse at what time most of the people travel so that they can pool in maximum number of cabs at that particular time.At peak hours which from analysis we got to know as 6 PM in the evening and morning 8-10AM the company can come out with some attractive offers and prices to attract maximum number of customers.Also they can come up with car-pooling option to obtain more number of bookings at a given point of time."
      ],
      "metadata": {
        "id": "t7dguLynrHcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Taxi Trip duration vs Distance(Bivariate)"
      ],
      "metadata": {
        "id": "xnMZvg2orIPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Visualizing Percentage of customers taken international plan\n",
        "sns.set_style('white')\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['figure.figsize'] = (8,8)\n",
        "plt.xlim(0,100000)\n",
        "plt.ylim(0,600)\n",
        "sns.scatterplot(x=\"trip_duration\", y=\"distance\", data=df)"
      ],
      "metadata": {
        "id": "BIKEKoqErIPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "mwHQVoIQrIPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables in a data set.\n",
        "I used scatter plot to check how the 'distance' variable is related to the 'trip_duration' variable."
      ],
      "metadata": {
        "id": "_OS83_DorIPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "Kwy5hImkrIPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot I came to know that 'distance' column and 'trip_duration' are not linearly realted.There were lot of trips whose distance was very minimal nearing to '0' but the time taken to cover them was ranging from 20,000 to 80,000 seconds.Also we can see that in few of the trips very large distance like 600kms were covered in very short period of time which is very unlikley.There are many trips with distance as '0' but have clocked time.This is very unlikely to happen and can be removed. "
      ],
      "metadata": {
        "id": "v29pTB30rIPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "0sjxqkK0rIPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the analysis found will definetly have a positive impact on the business.\n",
        "As a cab owner it will be very important to analyse these data of the trip such as what is time taken to cover a particular  distance .It helps to analyse if there is any delay in reaching the destination and the reasons behind it.The reasons can be traffic,bad roads ,bad wheather or even bad driving."
      ],
      "metadata": {
        "id": "q3Py5eq2rIPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 -Month of booking vs number of booking.(Bivariate)"
      ],
      "metadata": {
        "id": "y7avNpR9HL5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6  visualization code\n",
        "# vizualizing code for the months having more number of bookings.\n",
        "sns.set_style('white')\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['figure.figsize'] = (8,8)\n",
        "group1=df.groupby(\"pickup_month\")[\"id\"].count().reset_index().sort_values(by=\"id\" , ascending=False).rename(columns={'id':'Number of Bookings'})\n",
        "group1"
      ],
      "metadata": {
        "id": "XupyxJMCHL50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pointplot(x=\"pickup_month\" , y=\"Number of Bookings\" , data=group1 , color=\"indigo\").set(title=\"Number of bookings for each month\")"
      ],
      "metadata": {
        "id": "80ihK0WOIeG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Z5yPuW4iHL50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Points plots are very usefull in focusing  and explaining on the different levels of comparison of two variables.I used point plot to analyse how the bookings are done during various months of a year. "
      ],
      "metadata": {
        "id": "e2Vj3xyvHL50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "3vIdo55JHL50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot I could visualize that the maximum number of cab bookings were done in the months of 'March' , 'April' and May.\n",
        "Comparitively less bookings are done in the months of 'January' and 'June'.This can be due to weather conditions.\n"
      ],
      "metadata": {
        "id": "_TwmLyT0HL51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "IlhlbgNBHL51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,this analysis will definetly have a positive impact on the business.\n",
        "The taxi owners and vendors can use this data to come up with more number vehicles during these periods.Also during the months where cab bookings are less the companies can come up with attractive fares and special offers to improve their business."
      ],
      "metadata": {
        "id": "zecN6W5IHL51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart-7 ##Determining the number of pickups done on each day of the week(Univariate)"
      ],
      "metadata": {
        "id": "9i56ujKV-Ogy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7  visualization code\n",
        "# vizualizing code for the number of pick ups and drop offs done on each day of the week.\n",
        "figure, ax = plt.subplots(nrows=2, ncols = 1, figsize = (10,10))\n",
        "sns.countplot(x = 'pickup_day', data = df, ax = ax[0])\n",
        "ax[0].set_title('no of pickups done on each day of the week')\n",
        "\n",
        "sns.countplot(x = 'dropoff_day', data = df, ax=ax[1])\n",
        "ax[1].set_title('no of dropoffs done on each day of the week')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "kvLVN_t-bpSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "yxjklrqffwwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots are used to show different segments of information.The vertical bars in the bar plot are very usefull to compare different discrete and categorical variables.I used bar plot to visualise the number of pick_ups and drop_offs happening on each day of the week. "
      ],
      "metadata": {
        "id": "wrSF4YlPfwwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "t2v8hxNAfwwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above plot day-'0' represents 'Monday' and is considerd as the beginning of the week. day-'6' represents Sunday,the end of the week.From the above plot we can see that the number of pickups and dropoffs are maximum for 'Friday' which is represented as day-'4'. From the above plot I came to know that he maximum number of pick_ups and drop_offs happened on day-4 which is Friday.\n",
        "The minimum number of pick_ups and drop_offs happened on Monday and Sunday represented as days '0' and '6' respectively."
      ],
      "metadata": {
        "id": "KudGZcEOCzjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "YveDNi1QfwwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,this analysis will definetly have a positive impact on the business.\n",
        "The taxi owners and vendors can use this data to come up with more number vehicles during these periods.Also during the months where cab bookings are less the companies can come up with attractive fares and special offers to improve their business."
      ],
      "metadata": {
        "id": "4dg06vUhfwwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8- Determining the relationship between 'Trip_duration' and other variables."
      ],
      "metadata": {
        "id": "oQ-pHCWMHO92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "col=df.describe().columns\n",
        "col"
      ],
      "metadata": {
        "id": "Eb_s6DRSHO92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in col:\n",
        "  #fig,ax=plt.subplots(figsize=(10,6))\n",
        "  sns.relplot(data=df, x=df[col], y=\"trip_duration\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W3b-E6c4crFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "DEMFeE18HO92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Seaborn Relational Plot (relplot) allows us to visualise how variables within a dataset  are related to each other.It uses scatter plot and when specified line plot to visualize the relationship between the variables in the dataset.I used Relational plot to visualise the realationship present between the dependent variable 'trip_duration' and various other independent variables present in the NYC taxi trip data set. "
      ],
      "metadata": {
        "id": "zyFFR4BgHO92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "MR3eS7xTHO93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plots we can see the relatioship between the dependent variable 'trip_duration' and other independent variables.From the plots we can figure out that there is no much relationship or linear relationship between the dependent variable and other independent variables.We can only see a slight linear relationship between 'trip_duration' and 'distance'.For rest of all the independent variables there is no  relationship  with target variable."
      ],
      "metadata": {
        "id": "0neckRpZHO93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JmI1Y4YbHO93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relation plot is to only visualize the relation between the two variables in the dataset.It doesnt have any contribution to the business .\n"
      ],
      "metadata": {
        "id": "OgeOEDgIHO93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Corelation-HeatMap"
      ],
      "metadata": {
        "id": "XGShqbS2HSAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9- visualization code\n",
        "# Visualizing the corelation between the variables.\n",
        "# Correlation Heatmap visualization code\n",
        "\n",
        "plt.figure(figsize = (15,7))\n",
        "sns.heatmap(df.corr(), linewidths = 0.5, annot = True ,cmap=\"crest\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-6GTfuViHSAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "B4XxyvTzHSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heat Maps are graphical representations of data that utilize color-coded systems where values are depicted by color.It represents the coefficients to visualize the strength of correlation among variables. It helps find features that are best for Machine Learning model building.It transforms the correlation matrix into color coding .Each cell in the heatmap shows the corelation  between two variables.The corelation values varies from the range [-1,1]"
      ],
      "metadata": {
        "id": "xBXFKddDHSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "Mal1wQlpHSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot I could infer that there is a high corelation between 'pick_up_time_hour' and 'dropoff_time_hour' which is around 93%.There is also a high corelation between  'pickup_day' and dropoff_day' which is around 99%.There is a slight corelation between 'pickup_longitude' and 'dropoff_longitude' which is around 78%.There is also a slight corelation between the variables 'pickup_latitude' and 'dropoff_latitude' which is around 49% "
      ],
      "metadata": {
        "id": "pxwDuAqQHSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "BgP_e3WlHSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We use HeatMap only to check the corelation between the variables in the datset.This analysis will be very usefull in futher creating machine learning models.However this analysis will not contribute anything to business."
      ],
      "metadata": {
        "id": "15jESfbyHSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 -Histogram plot and Box plot for  the features of the dataset"
      ],
      "metadata": {
        "id": "I3aNiIh9fKqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy.sql.base import elements\n",
        "# Chart - 9 visualization code\n",
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "for c in df.describe().columns:\n",
        "  chart=plt.figure(figsize=(9,6))\n",
        "  ax=chart.gca()\n",
        "  feature= (df[c])\n",
        "  sns.distplot(df[c])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed')\n",
        "  ax.axvline(feature.median(),color='blue', linestyle='dashed')\n",
        "\n",
        "  ax.set_title(c)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualizing code of box plot for each columns to know the data distibution\n",
        "for i in df.describe().columns:\n",
        "    image = plt.figure(figsize=(9, 6))\n",
        "    ax = image.gca()\n",
        "    sns.boxplot(df[i])\n",
        "    ax.set_title('Label by ' + i)\n",
        "    #ax.set_ylabel(\"Churn\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WPVINT_UfKqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "hTF1e9PwfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram is used to illustrate the major features of the distribution of the data in a convenient form It shows the frequency of numerical data using rectangles. The height of a rectangle (the vertical axis) represents the distribution frequency of a variable (the amount, or how often that variable appears).\n",
        "\n",
        "Thus, I used the histogram plot to analyse the variable distributions over the whole dataset and to undersdtand its symmetricity.\n",
        "\n",
        "Box  plots are very effective and easy to read, as they can summarize data from multiple sources and display the results in a single graph. Box and whisker plots allow for comparison of  data from different categories for easier, more effective decision-making by offering general information about a group of data's symmetry, skew, variance, and outliers.\n",
        "\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "XOYaZpKpfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "1xrBLU6IfKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all columns except 'pickup_time_hour' and dropoff_time_hour'are symmetrically distributed.However for most of the columns we can see that mean and median do not coincide.\n",
        "In the boxplot we can see that  few of the columns like 'passenger_count' , 'trip_duration' , 'distance' , 'pickup_latitude'  , 'pickup_longitude' , 'dropoff_latitude' and 'dropoff_longitude' has outliers which has to be treated."
      ],
      "metadata": {
        "id": "rrwjuBlHfKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "yiLHMWFSfKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just a histogram and box plot cannot define business impact. It's done just to see the distribution of the column data over the dataset."
      ],
      "metadata": {
        "id": "7cTtRDwufKqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df , diag_kind=\"hist\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Seaborn Pairplot allows us to plot pairwise relationships between variables within a dataset. This creates a nice visualisation and helps us understand the data by summarising a large amount of data in a single figure.\n",
        "\n",
        "Therefore, I used pair plot to analyse the patterns of data and realationship between the features. It is very much same as the correlation map but here we can see the graphical representation of relationship between the pairs of vaariables."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart I got to know that, there is a very less linear relationship between the variables of the dataset."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Customers not churning with Voicemail plan have sent average of at least 30 number of voicemails.\n",
        "2. Customers not churning having  customer care calls average of at most 2.\n",
        "3. Customers churning have total average call minutes including day, evening, night and international calls is 215."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variance = lambda x : sum([(i - np.mean(x))**2 for i in x])/(len(x)-1)\n",
        "zcdf = lambda x: norm(0,1).cdf(x)\n",
        "# Creating a function for getting P value\n",
        "def p_value(z,tailed,t,hypothesis_number,df,col):\n",
        "  if t!=\"true\":\n",
        "    z=zcdf(z)\n",
        "    if tailed=='left':\n",
        "      return z\n",
        "    elif tailed == 'right':\n",
        "      return 1-z\n",
        "    elif tailed == 'two_tailed':\n",
        "      if z>0.5:\n",
        "        return 2*(1-z)\n",
        "      else:\n",
        "        return 2*z\n",
        "    else:\n",
        "      return np.nan\n",
        "  else:\n",
        "    z,p_value=stats.ttest_1samp(df[col],hypothesis_number)\n",
        "    return p_value\n",
        "# Conclusion about the P - Value\n",
        "def conclusion(p):\n",
        "  significance_level = 0.05\n",
        "  if p>significance_level:\n",
        "    return f\"Failed to reject the Null Hypothesis for p = {p}.\"\n",
        "  else:\n",
        "    return f\"Null Hypothesis rejected Successfully for p = {p}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "9C3DDHpRUM7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "Out of a sample of 500  taxis booked it is found that 55% of them were booked by vendor_id: 2 .If the claim is that majority of bookings were made by vendor_id:2, find its test statistic"
      ],
      "metadata": {
        "id": "VmF_OdmmCJvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "d5ybnedqCJvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D9ZjnwEHmSQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: Ho = 50\n",
        "\n",
        "Alternate Hypothesis : Ha > 50\n",
        "\n",
        "Test Type: Right Tailed Test\n"
      ],
      "metadata": {
        "id": "_mM-z37JCJvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "sample=df[(df[\"vendor_id\"]==2)]\n",
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 50\n",
        "n=500\n",
        "actual_value=55\n"
      ],
      "metadata": {
        "id": "OW1ztdDZCJvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Z value\n",
        "z = (actual_value - hypothesis_number)/math.sqrt(hypothesis_number)*(1-hypothesis_number)/n\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='right',t=\"false\",hypothesis_number=hypothesis_number,df=sample,col=\"vendor_id\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "h4ehMHwXCJvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-Test as the statistical test to obtain P-Value and found out the P-value to be 0.87 which is greater than the significance level.\n",
        "Hence deriving the conclusion as \"Failed To reject the Null Hypothesis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for required columns to know the data distibution\n",
        "\n",
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= sample[\"vendor_id\"]\n",
        "sns.distplot(feature)\n",
        "ax.axvline(feature.mean(),color='red', linestyle='dashed', linewidth=3)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title(\"Dist plot for vendor_id\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tz1TkC-_ZdhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the figure the mean and median are coinciding. Thus, it is a Normal Distribution. That's why I have used Z-Test directly."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "Average number of passengers in a taxi is 2"
      ],
      "metadata": {
        "id": "jLrtcv83OEmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "5djnGilGOEmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : Ho = 2\n",
        "\n",
        "Alternate Hypothesis : Ha != 2\n",
        "\n",
        "Test Type : Two Tailed Test\n",
        "\n"
      ],
      "metadata": {
        "id": "ps8ZluDUOEmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "DADo_qtwOEmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "sample_2=df[(\"passenger_count\")].sample(n=50)\n",
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 2\n"
      ],
      "metadata": {
        "id": "0v4QJexjOEmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2.shape"
      ],
      "metadata": {
        "id": "bwOycsARblyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_mean = sample_2.mean()\n",
        "size = len(sample_2)\n",
        "std=(variance(sample_2))**0.5\n",
        "n=50"
      ],
      "metadata": {
        "id": "bFmQQ9ppbg08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Z value\n",
        "z = (sample_mean - hypothesis_number)*math.sqrt(n)/std \n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='two_tailed',t=\"false\",hypothesis_number=hypothesis_number,df=sample_2 , col=\"passenger_count\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "40AHeMyBe-Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "69qjvWOxOEmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference=sample_2.mean()-sample_2.median()\n",
        "print(\"Mean Median Difference is :-\",mean_median_difference)"
      ],
      "metadata": {
        "id": "4VWq37ORjNaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_2.mean())\n",
        "print(sample_2.median())\n"
      ],
      "metadata": {
        "id": "-zPTwzrvhcD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "0VL2QIwzOEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-Test as the statistical testing to obtain P-Value and found the result that Null hypothesis can't be rejected and Customers not churning having customer care calls average of at most 2. So, we can see a clear picture that average of 2-2.5 doesn't lead to customer churn. So, we should keep the customer queries resolution call average in betwwen 2-2.5."
      ],
      "metadata": {
        "id": "9EZtE-psOEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above the Mean median difference is around 0.4 and nearly zero. Mean is approximately same as the median. Thus, it is a Normal Distribution. That's why I have used Z-Test directly."
      ],
      "metadata": {
        "id": "1WtUDDNROEmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3\n",
        "Out of the various trips, the average trip duration is 600 seconds"
      ],
      "metadata": {
        "id": "FqZJ_xAgOH3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "pE65w-N1OH3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : mean = 600\n",
        "\n",
        "Alternate Hypothesise : mean != 600\n",
        "\n",
        "Type of Test : Two Tailed test"
      ],
      "metadata": {
        "id": "9QVaMT2vOH3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "OISwWHvhOH3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value by taking a sample of 80 bookings.\n",
        "sample_3=df[\"trip_duration\"].sample(n=80)\n",
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 600\n",
        "n=80\n"
      ],
      "metadata": {
        "id": "yOMTknz0OH3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z,p_value=stats.ttest_1samp(df[\"trip_duration\"],hypothesis_number)"
      ],
      "metadata": {
        "id": "oTCXAB3-9eEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(p_value)\n",
        "print(z)"
      ],
      "metadata": {
        "id": "p4wlYgm9-X39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "significance_level = 0.05\n",
        "if p_value >significance_level:\n",
        "  print(f\"Failed to reject the Null Hypothesis for p = {p_value}.\")\n",
        "else:\n",
        "  print(f\"Null Hypothesis rejected Successfully for p = {p_value}\")"
      ],
      "metadata": {
        "id": "VMUAJXfyM_QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "rheC10M4OH3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used T test to perform hypothesis testing.I found out that the p_value is less than the significance level .Hence Null hypothesis rejected sucessfully."
      ],
      "metadata": {
        "id": "Obd1DMgROH3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "-CtoKDX7OH3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_3.mean())\n",
        "print(sample_3.median())"
      ],
      "metadata": {
        "id": "5yNwSbN31sNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference=sample_3.median()- sample_3.mean()\n",
        "print(\"Mean Median Difference is :-\",mean_median_difference)"
      ],
      "metadata": {
        "id": "8a7kQaT2m054"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above code we can see that there is a huge difference between mean and median.Hence the data is skewed and not Normallay distributed.We cannot perform z-test for skewed data.\n",
        "\n",
        "Non-parametric tests are most useful for small studies. Using non-parametric tests in large studies may provide answers to the wrong question, thus confusing readers. For studies with a large sample size, t-tests  can and should be used even for heavily skewed data.\n",
        "\n",
        "So, for a skewed data we can use T-test for better result. Thus, I used t - test."
      ],
      "metadata": {
        "id": "f9IOMHYuOH3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for further feature engineering\n",
        "new_df=df.copy()\n",
        "new_df.shape"
      ],
      "metadata": {
        "id": "nEQ5IKIFtrok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df"
      ],
      "metadata": {
        "id": "jIT3Hlk0rCyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.dtypes"
      ],
      "metadata": {
        "id": "Q86V2GYVo9hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# To separate the symmetric distributed features and skew symmetric distributed features\n",
        "symmetric_features=[]\n",
        "non_symmetric_features=[]\n",
        "for i in new_df.describe().columns:\n",
        "  if abs(new_df[i].mean()-new_df[i].median())<0.1:\n",
        "    symmetric_features.append(i)\n",
        "  else:\n",
        "    non_symmetric_features.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_features)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_features)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqJ-nKe-7sH1"
      },
      "source": [
        "# For Symmetric features defining upper and lower boundry\n",
        "def outlier_detection_symmetric(df0,feature):\n",
        "  print(feature)\n",
        "  upper_boundary= (df0[feature].mean())+3*(df0[feature].std())\n",
        "  lower_boundary= (df0[feature].mean())-3*(df0[feature].std())\n",
        "  print(upper_boundary,lower_boundary)\n",
        "  return upper_boundary,lower_boundary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu4N8d5i8Jdr"
      },
      "source": [
        "# Restricting the data to lower and upper boundry\n",
        "for feature in symmetric_features :\n",
        " upper_limit ,lower_limit=outlier_detection_symmetric(df,feature)\n",
        " print(\"upper limit for feature\" , feature,\"=\" ,upper_limit)\n",
        " print(\"lower limit for feature\" , feature,\"=\",lower_limit)\n",
        " outlier_data=new_df[new_df[feature] <= lower_limit]\n",
        " outlier_data=new_df[new_df[feature] >= upper_limit]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above code we can infer that for the features 'year' , 'pickup_day' ,'dropoff_day' the values lies well within the range.Hence we will do the outlier treatment for rest of the features"
      ],
      "metadata": {
        "id": "EbK8Esypaewm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#outlier treatment for the feature 'pickup_latitude'\n",
        "#from the above code we came to know that upper limit for pickup_latitude feature is :40.84 and lower limit is:40.65 .\n",
        "new_df=new_df[new_df['pickup_latitude'] > 40.65 ]\n",
        "new_df=new_df[new_df['pickup_latitude']  < 40.84]"
      ],
      "metadata": {
        "id": "4xI5beBiWRRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "M4as_UEKW0q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outlier treatment for the feature 'pickup_longitude'\n",
        "#from the above code we came to know that upper limit for pickup_longitude feature is :-73.76 and lower limit is:-74.18 .\n",
        "new_df=new_df[new_df['pickup_longitude'] > -74.18 ]\n",
        "new_df=new_df[new_df['pickup_longitude']  < -73.76]"
      ],
      "metadata": {
        "id": "f8rWSMM9W8VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "M_2S92xlX-xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outlier treatment for the feature 'dropoff_latitude'\n",
        "#from the above code we came to know that upper limit for dropoff_latitude feature is :40.85 and lower limit is:40.64 .\n",
        "new_df=new_df[new_df['dropoff_latitude'] > 40.64 ]\n",
        "new_df=new_df[new_df['dropoff_latitude']  < 40.85]"
      ],
      "metadata": {
        "id": "GvmBdjICYSas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "a345IcAoYPlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outlier treatment for the feature 'dropoff_longitude'\n",
        "#from the above code we came to know that upper limit for dropoff_longitude feature is :-73.76 and lower limit is:-74.18 .\n",
        "new_df=new_df[new_df['dropoff_longitude'] > -74.18 ]\n",
        "new_df=new_df[new_df['dropoff_longitude']  < -73.76]"
      ],
      "metadata": {
        "id": "CJe5hNpPZJun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "wuIuCp7yZu9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atgeFnRY8G1w"
      },
      "source": [
        "# For Skew Symmetric features defining upper and lower boundry\n",
        "#Outer Fence\n",
        "def outlier_detection_skewedfeature(df1,feature):\n",
        "  df1.sort_values(by=feature)\n",
        "  IQR= df1[feature].quantile(0.75)- df1[feature].quantile(0.25)\n",
        "  lower_limit=df1[feature].quantile(0.25)-(1.5*IQR)\n",
        "  upper_limit =df1[feature].quantile(0.75)+(1.5*IQR)\n",
        "  return upper_limit,lower_limit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundry\n",
        "for c in non_symmetric_features :\n",
        "  upper_limit_outlier , lower_limit_outlier=outlier_detection_skewedfeature(df,c)\n",
        "  print(\"lower_limit for the feature=\" , c,lower_limit_outlier)\n",
        "  print(\"upper_limit for the feature=\" ,c, upper_limit_outlier)\n"
      ],
      "metadata": {
        "id": "gP4UZkA4adYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the features 'vendor_id' , 'dropoff_time_hour' , 'pick_up_time_hour' , 'dropoff_time_min', 'pick_up_time_min' , 'dropoff_time_min' the values lies well in between the upper and lower limit range.Hence we need to detect and treat outliers for rest of the features."
      ],
      "metadata": {
        "id": "NFV_6HQSAGpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#outlier treatment for the feature 'distance'\n",
        "#from the above code we came to know that upper limit for distance feature is :7.84 and lower limit is:-2.73 .\n",
        "new_df=new_df[new_df['distance'] > 0 ]#as distance can not be negative\n",
        "new_df=new_df[new_df['distance']  < 7.8]"
      ],
      "metadata": {
        "id": "WKHoiY_tGTHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "36RByIDMQIOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outlier treatment for the feature 'trip_duration'\n",
        "#from the above code we came to know that upper limit for distance feature is :2092 and lower limit is:-620 .\n",
        "new_df=new_df[new_df['trip_duration'] > 0]# as trip duration cannot be negative\n",
        "new_df=new_df[new_df['trip_duration'] < 2092]\n"
      ],
      "metadata": {
        "id": "pIGxmE8xBCha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "AuI6mkqTGHit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outlier treatment for the feature 'passenger_count'\n",
        "#from the above code we came to know that upper limit for distance feature is :3.5 and lower limit is:-0.5 .\n",
        "new_df=new_df[new_df['passenger_count'] > 0] #as passenger count cannot be nagative\n",
        "new_df=new_df[new_df['passenger_count'] < 3.5]\n"
      ],
      "metadata": {
        "id": "ku1sHTjbKpzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "JJrkyXi8MpvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df"
      ],
      "metadata": {
        "id": "hMneT027Syto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After Outlier Treatment showing the dataset distribution using box plot\n",
        "# Visualising  code for the numerical columns \n",
        "for col in new_df.describe().columns:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  sns.boxplot(new_df[col])"
      ],
      "metadata": {
        "id": "56KLPEcOw-MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I separeted the symmetric and skew symmetric features in my dataset.Then I defined the upper limit and lower limits for each variable.Further I restricted the highest value of each column to the upper limit and the lowest value of each feature to the lower limit.Presence of outliners may cause problems during model fitting.Hence to avoid negative influence to the training process of machine learning algorithms and statistical analysis , outliner treatement is necessary."
      ],
      "metadata": {
        "id": "IsFIXQE1gkCp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDVHj6krj3xX"
      },
      "source": [
        "In a Normal distribution where we have a symmetric curve and outliners are present , we can set the upper and lower boundaries by taking standard deviation into cosideration.\n",
        "\n",
        "For data which is skewed and dosent have a symmetric curve we can detect outliers using IQR method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdEiDCi_kAFM"
      },
      "source": [
        "Outlier detection using IQR method:\n",
        "IQR is used to measure variability by dividing a data set into quartiles. The data is sorted in ascending order and split into 4 equal parts. Q1, Q2, Q3 called first, second and third quartiles are the values which separate the 4 equal parts.\n",
        "\n",
        "Q1 represents the 25th percentile of the data.\n",
        "Q2 represents the 50th percentile of the data.\n",
        "Q3 represents the 75th percentile of the data.\n",
        "If a dataset has 2n / 2n+1 data points, then\n",
        "\n",
        "Q1 = median of the dataset.\n",
        "\n",
        "Q2 = median of n smallest data points.\n",
        "\n",
        "Q3 = median of n highest data points.\n",
        "\n",
        "IQR is the range between the first and the third quartiles namely Q1 and Q3: IQR = Q3  Q1. \n",
        "The data points which fall below Q1  1.5 IQR (lower limit) or above Q3 + 1.5 IQR(Upper limit)  are outliers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_trip=new_df.copy()"
      ],
      "metadata": {
        "id": "h_NGmmy4aG_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns=list(set(new_df.columns.to_list()).difference(set(new_df.describe().columns.to_list())))\n",
        "categorical_columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding 'Store_and_fwd_flag' using One Hot Encoding\n",
        "taxi_trip=pd.get_dummies(taxi_trip,columns = ['store_and_fwd_flag'])"
      ],
      "metadata": {
        "id": "2cHDDpdnzlqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_trip.head(3)"
      ],
      "metadata": {
        "id": "3tjNK6xp1yxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have encoded the 'Store and Fwd Flag' feature using 'One hot encoding' method.This feature provides information of wether a vehicle stores the ride details before sending it to the vendor.It has two unique values 'Y' and 'N'.Since the feature has only two unique values which are not ordinal,I have used 'One hot encoding' method to encode this categorical feature."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "**There are no text columns in the given dataset which I am working on. So, Skipping this part.**"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_trip['trip_duration_hour']=df['trip_duration']/3600"
      ],
      "metadata": {
        "id": "fb9dRLTd725E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_trip['trip_duration_hour']"
      ],
      "metadata": {
        "id": "ulx6Ecsk6be1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_trip['speed']=taxi_trip['distance']/taxi_trip['trip_duration_hour']\n",
        "taxi_trip['speed'].reset_index"
      ],
      "metadata": {
        "id": "6908qdf97H03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created new column called 'speed'."
      ],
      "metadata": {
        "id": "cil7E2PITdIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_trip.shape"
      ],
      "metadata": {
        "id": "zLVW007MTvlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape of dataset\n",
        "taxi_data=taxi_trip.copy()\n",
        "taxi_data.shape"
      ],
      "metadata": {
        "id": "PLWgA-MabkJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data.drop(columns={'id' ,'pickup_datetime' ,'dropoff_datetime','trip_duration_hour'} , axis=1 ,inplace=True)\n"
      ],
      "metadata": {
        "id": "m76C7i9Ebx5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data.head()"
      ],
      "metadata": {
        "id": "lzV0CqYAbP0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data.dtypes"
      ],
      "metadata": {
        "id": "kOK9m919dEIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to drop the constant features by Quasi Constant Feature\n",
        "from  sklearn.feature_selection import VarianceThreshold\n",
        "variance_threshold= VarianceThreshold(threshold=0.05)\n",
        "variance_threshold.fit(taxi_data)\n",
        "variance_threshold.get_support()\n",
        "col = [c for c in taxi_data.columns if c not in taxi_data.columns[variance_threshold.get_support()]]\n",
        "if \"trip_duration\" in col:\n",
        "  col.remove(\"trip_duration\")\n",
        "else:\n",
        "  pass\n",
        "taxi_data_changed=taxi_data.drop(col,axis=1)\n",
        "  "
      ],
      "metadata": {
        "id": "wXDqOMA7Ym-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape after feature dropped\n",
        "taxi_data_changed.shape"
      ],
      "metadata": {
        "id": "2037ugSmblqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data_changed.columns"
      ],
      "metadata": {
        "id": "yL9s03k_iC33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting  column names\n",
        "dataset_columns_required=taxi_data_changed.columns.to_list()\n",
        "dataset_columns_required.extend(['id','pickup_datetime','dropoff_datetime'])\n",
        "dataset_columns_required"
      ],
      "metadata": {
        "id": "KRz_X9jPUDe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "corr = taxi_data_changed.corr()\n",
        "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
        "\n",
        "def magnify():\n",
        "    return [dict(selector=\"th\",\n",
        "                 props=[(\"font-size\", \"7pt\")]),\n",
        "            dict(selector=\"td\",\n",
        "                 props=[('padding', \"0em 0em\")]),\n",
        "            dict(selector=\"th:hover\",\n",
        "                 props=[(\"font-size\", \"12pt\")]),\n",
        "            dict(selector=\"tr:hover td:hover\",\n",
        "                 props=[('max-width', '200px'),\n",
        "                        ('font-size', '12pt')])\n",
        "]\n",
        "\n",
        "corr.style.background_gradient(cmap, axis=1)\\\n",
        "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
        "    .set_caption(\"Hover to magify\")\\\n",
        "    .set_precision(2)\\\n",
        "    .set_table_styles(magnify())"
      ],
      "metadata": {
        "id": "mesGLFbqTV-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Variable Inflation Factor\n",
        "# the independent variables set\n",
        "X = taxi_data_changed.copy()\n",
        "  \n",
        "# VIF dataframe\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "  \n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
        "                          for i in range(len(X.columns))]\n",
        "print(vif_data)\n",
        "  \n",
        "for i in range(len(vif_data)):\n",
        "  vif_data.loc[i,\"VIF\"]=vif_data.loc[i,\"VIF\"].round(2)\n",
        "  if vif_data.loc[i,\"VIF\"]>=8:\n",
        "    print(vif_data.loc[i,\"feature\"])\n"
      ],
      "metadata": {
        "id": "s6ekJylYVViu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the collineariaty of all features and finding multicolinearity\n",
        "def feature_correlation(df2,threshold):\n",
        "  column_correlation=set()\n",
        "  corr_matrix= df2.corr()\n",
        "  for i in range (len(corr_matrix.columns)):#i stores the number of rows in matrix\n",
        "    for j in range(i):#j stores the number of columns in matrix\n",
        "      if abs (corr_matrix.iloc[i,j])>threshold:#checking the threshold value of each value in matrix \n",
        "        colmn=corr_matrix.columns[i]\n",
        "        column_correlation.add(colmn)\n",
        "  return list(column_correlation)#returns the list of columns whose collinearity value is greater than the threshold,which are highly corelated.\n",
        "\n"
      ],
      "metadata": {
        "id": "_RD2Mq1SAZM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data_changed.columns"
      ],
      "metadata": {
        "id": "eN0QqicdiobN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting multicolinear columns and dropping them \n",
        "highly_correlated_columns=feature_correlation(taxi_data_changed,0.7)\n",
        "if \"trip_duration\"  in highly_correlated_columns:#checking if target variable is there in the list of corelated columns\n",
        "  highly_correlated_columns.remove(\"trip_duration\")\n",
        "if \"distance\" in highly_correlated_columns:\n",
        "  highly_correlated_columns.remove(\"distance\")\n",
        "if \"speed\" in highly_correlated_columns:\n",
        "  highly_correlated_columns.remove(\"speed\")\n",
        "#we are dropping the highly corelated columns\n",
        "print(highly_correlated_columns)\n",
        "# As the features \"distance\" , :\"speed\" are important features and \"trip duration\" is target variable we are not taking it in \"highly correlated columns\""
      ],
      "metadata": {
        "id": "f2HYgia8TCCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data_changed.drop(highly_correlated_columns,axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "OhDGv2eClNJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data_changed.columns"
      ],
      "metadata": {
        "id": "BKotYlYflOTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation after dropping the required columns\n",
        "# Correlation Heatmap visualization code\n",
        "corr = taxi_data_changed.corr()\n",
        "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
        "\n",
        "def magnify():\n",
        "    return [dict(selector=\"th\",\n",
        "                 props=[(\"font-size\", \"7pt\")]),\n",
        "            dict(selector=\"td\",\n",
        "                 props=[('padding', \"0em 0em\")]),\n",
        "            dict(selector=\"th:hover\",\n",
        "                 props=[(\"font-size\", \"12pt\")]),\n",
        "            dict(selector=\"tr:hover td:hover\",\n",
        "                 props=[('max-width', '200px'),\n",
        "                        ('font-size', '12pt')])\n",
        "]\n",
        "\n",
        "corr.style.background_gradient(cmap, axis=1)\\\n",
        "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
        "    .set_caption(\"Hover to magify\")\\\n",
        "    .set_precision(2)\\\n",
        "    .set_table_styles(magnify())"
      ],
      "metadata": {
        "id": "Lq_eSM7Ldbsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Again checking VIF after dropping highly corelated features\n",
        "# the independent variables set\n",
        "X = taxi_data_changed.copy()\n",
        "  \n",
        "# VIF dataframe\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "  \n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
        "                          for i in range(len(X.columns))]\n",
        "vif_data\n",
        "  \n",
        "for i in range(len(vif_data)):\n",
        "  vif_data.loc[i,\"VIF\"]=vif_data.loc[i,\"VIF\"].round(2)\n",
        "  if vif_data.loc[i,\"VIF\"]>=8:\n",
        "    print(vif_data.loc[i,\"feature\"])\n"
      ],
      "metadata": {
        "id": "KvzzTWXbe3QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data_changed.drop(\"vendor_id\" , axis=1 ,inplace=True)\n",
        "#We are not deleting the columns trip duration , distance and speed as they are very important. Also feature 'Trip_duration' is target variable."
      ],
      "metadata": {
        "id": "ZOF4spj9Srnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After Feature Selection checking the shape left with\n",
        "taxi_data_changed.shape"
      ],
      "metadata": {
        "id": "T7_-rO-0fi9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Dropping Constant Feature, Dropping columns having multicolinearity and validate through VIF. \n",
        "\n",
        "Feature Selector that removes all low variance features. This feature selection algorithm looks only at the features(X), not the desired outputs(Y), and can be used for unsupported learning.\n",
        "\n",
        "A Pearson correlation is a number between -1 and 1 that indicates the extent to which two variables are linearly related. The Pearson correlation is also known as the product moment correlation coefficient (PMCC) or simply correlation\n",
        "\n",
        "Pearson correlations are suitable only for metric variables The correlation coefficient has values between -1 to 1\n",
        "\n",
        " A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
        "\n",
        " A value closer to 1 implies stronger positive correlation\n",
        "\n",
        " A value closer to -1 implies stronger negative correlation\n",
        "\n",
        "Collinearity is the state where two variables are highly correlated and contain similar information about the variance within a given dataset. To detect collinearity among variables, simply create a correlation matrix and find variables with large absolute values.\n",
        "\n",
        "Steps for Implementing VIF\n",
        "\n",
        " Calculate the VIF factors.\n",
        "\n",
        " Inspect the factors for each predictor variable, if the VIF is between 510, multicollinearity is likely present and you should consider dropping the variable.\n",
        "\n",
        "In VIF method, we pick each feature and regress it against all of the other features. For each regression, the factor is calculated as :\n",
        "\n",
        "VIF=\\frac{1}{1-R^2}\n",
        "\n",
        "Where, R-squared is the coefficient of determination in linear regression. Its value lies between 0 and 1.\n",
        "\n",
        "1st I dropped columns having constant or quasi constant variance. Then using pearson corelation I removed the columns having multicolinearity and again validate the VIFs for each feauture and found some features having VIF of more than 5-10 and I considered it to be 8 and again manipulated some features and again dropped multicolinear columns to make the VIF less than 8. The features got decreased from 24 to 10."
      ],
      "metadata": {
        "id": "8_syLORmZO_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedded Method of validating the feature importances of selected features\n",
        "def randomforest_embedded(x,y):\n",
        "  # Create the random forest eith hyperparameters\n",
        "  random_model= RandomForestRegressor(n_estimators=75)\n",
        "  # Fit the mmodel\n",
        "  random_model.fit(x,y)\n",
        "  # get the importance of thr resulting features\n",
        "  feature_importances= random_model.feature_importances_\n",
        "  # Create a data frame for visualization\n",
        "  features_df= pd.DataFrame({\"Features\": pd.DataFrame(x).columns, \"Importances\": feature_importances})\n",
        "  features_df.set_index('Importances')\n",
        "  # Sort in ascending order to better visualization\n",
        "  features_df= features_df.sort_values('Importances')\n",
        "  # Plot the feature importances in bars\n",
        "  # final_df.plot.bar(color='teal')\n",
        "  return features_df"
      ],
      "metadata": {
        "id": "NVL6Ha0lfyRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting feature importance of selected features\n",
        "randomforest_embedded(x=taxi_data_changed.drop([\"trip_duration\"],axis=1),y=taxi_data_changed[\"trip_duration\"])"
      ],
      "metadata": {
        "id": "7Kh30ZnTf0e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Random Forest embedded method to find out the importance of features.Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration.Embedded methods combines the advantageous aspects of both Filter and Wrapper feature selection methods.Embedded methods perform feature selection and training of the algorithm in parallel.Random Forest uses mean decrease impurity (Gini index) to estimate a features importance. The lower the value, the more important the feature is."
      ],
      "metadata": {
        "id": "2zBQh2_8awan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data_changed.shape"
      ],
      "metadata": {
        "id": "sEW6aKNE05sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting symmetric and skew symmetric features \n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for c in taxi_data_changed.describe().columns:\n",
        "  if abs(taxi_data_changed[c].mean()-taxi_data_changed[c].median())<0.1:\n",
        "    symmetric_feature.append(c)\n",
        "  else:\n",
        "    non_symmetric_feature.append(c)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "YnvslXZKj5dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data=taxi_data_changed.copy()"
      ],
      "metadata": {
        "id": "ceEWAROyuwma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data"
      ],
      "metadata": {
        "id": "ZDov3BDkoPg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_data_changed['trip_duration'].max()"
      ],
      "metadata": {
        "id": "vAracRyJWZ8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data['trip_duration']=np.log(nyc_data['trip_duration'])\n"
      ],
      "metadata": {
        "id": "sac3ukURl6CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data['trip_duration'].max()"
      ],
      "metadata": {
        "id": "FjNZjFFJWj_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data['distance']=np.log(nyc_data['distance'])\n"
      ],
      "metadata": {
        "id": "Kiq60PEO8L3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data['speed']=np.log(nyc_data['speed'])"
      ],
      "metadata": {
        "id": "DkdTZrcX8Pz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data['passenger_count']=np.log(nyc_data['passenger_count'])"
      ],
      "metadata": {
        "id": "RzrfqayTpADl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data['pickup_month']=np.log(nyc_data['pickup_month'])"
      ],
      "metadata": {
        "id": "o7VVAcfSpNhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data"
      ],
      "metadata": {
        "id": "hiER10y3nO2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "for col in nyc_data.loc[:,non_symmetric_feature]:\n",
        "  print(col)\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  feature= (nyc_data[col])\n",
        "  sns.distplot(feature , fit=norm)\n",
        "  ax=fig.gca()\n",
        "  ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)    \n",
        "  ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zdFioDnFkscQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above code I found out that there were eight features that werent following the guassian distribution.Thus I used the Log Trasformation technique to achieve guassian distribution for these features.I tried with  other techniques and found out that this Log transformation to be the more suitable one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yIaFNXkPGdL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the required column\n",
        "scaler=StandardScaler()\n",
        "nyc_data['pick_up_time_min'] = scaler.fit_transform(nyc_data['pick_up_time_min'].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "KvPL0rvG6aAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data['dropoff_time_min']=scaler.fit_transform(nyc_data['dropoff_time_min'].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "4u4gS0blcSB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nyc_data.drop('pick_up_time_min' , axis=1 , inplace=True)"
      ],
      "metadata": {
        "id": "aqs9zNLRdIc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data['dropoff_time_hour']=scaler.fit_transform(nyc_data['dropoff_time_hour'].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "3fE5L0gZdff5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nyc_data.drop('dropoff_time_hour' , axis=1 , inplace=True)"
      ],
      "metadata": {
        "id": "E8bWidm9efF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the dataset\n",
        "nyc_data.head()"
      ],
      "metadata": {
        "id": "vYT_PjFl96Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we are using an algorithm that assumes your features have a similar range, you should use feature scaling.Variables that are measured at different scales do not contribute equally to the model fitting & model learned function and might end up creating a bias.\n",
        "\n",
        "When the ranges of the features differ much then we should use feature scaling. If the range does not differ a lot for eaxample one of them is between 0 and 2 and the other one is between -1 and 0.5 then we can leave them as it's. However, you should use feature scaling if the ranges are, for example, between -2 and 2 and between -100 and 100.\n",
        "\n",
        "We Use Standardization when your data follows Gaussian distribution.\n",
        "We Use Normalization when your data does not follow Gaussian distribution.\n",
        "\n",
        "So,In my data 'pickup_time_min' , dropoff_time_min' , 'dropoff_time_hour' columns have large data difference and are following guassian distribution.Hence , I have done Standardization on these features using standardscaler().\n"
      ],
      "metadata": {
        "id": "-_-SaQemdYeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per my knowledge, for this dataset dimensionality reduction is not required.\n",
        "\n",
        "Dimensionality reduction brings many advantages to your machine learning data, including: Fewer features mean less complexity. We will need less storage space because we have fewer data. Fewer features require less computation time.\n",
        "\n",
        "It is commonly used in the fields that deal with high-dimensional data, such as speech recognition, signal processing, bioinformatics, etc. It can also be used for data visualization, noise reduction, cluster analysis, etc.It helps in reduction of highly corelated and redundant features.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        " # split into 80:20 ratio\n",
        " #Create the data of independent variables\n",
        "x= nyc_data.drop(\"trip_duration\" ,axis=1).values\n",
        "# Create the data of dependent variable\n",
        "y = nyc_data[\"trip_duration\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y , test_size = 0.2, random_state = 0)\n",
        "  \n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to build a reliable machine learning model, we need to split our dataset into the training , and test data set.\n",
        "Training sets are commonly used to estimate different parameters or to compare different model performance. The testing data set is used after the training is done. The training and test data are compared to check that the final model works correctly.\n",
        "The training set should not be too small; else, the model will not have enough data to learn. On the other hand, if the validation set is too small, then the evaluation metrics  will have large variance and will not lead to the proper tuning of the model.\n",
        "Since the data is is pretty huge I have split the data into 80:20 ratio.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data set is an example of Machine learning Regression , Handling Imbalanced dataset will not be needed."
      ],
      "metadata": {
        "id": "VMCo1-B-dL7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Model - 1 -Implementing Linear Regression"
      ],
      "metadata": {
        "id": "peAK6Cc_HQeo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbsRRWRfEQtK"
      },
      "source": [
        "# ML Model - 1 Implementation\n",
        "linear_reg = LinearRegression().fit(X_train, y_train)\n",
        "linear_reg.score(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF36F8oQEVIp"
      },
      "source": [
        "# Checking the coefficients\n",
        "linear_reg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUsxKejfytOD"
      },
      "source": [
        "# Checking the intercept value\n",
        "linear_reg.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_5ATxY7EbXx"
      },
      "source": [
        "# Predict on the model\n",
        "y_pred = linear_reg.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGww1WE54cUm"
      },
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlo2F3-p4hNC"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKnayE_g4khH"
      },
      "outputs": [],
      "source": [
        "predictions.sample(20).plot(kind='bar',figsize=(14,8))\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
        "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='orange')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plot is a bar plot that shows the actual and compared vales for each of the observations.Bar charts should be used when we are showing segments of information.Vertical bar charts are useful to compare different categorical or discrete variables, such as age groups, classes etc."
      ],
      "metadata": {
        "id": "_GBBLr1u-uVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "khncImPpHcol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-Lmo4I54xnb"
      },
      "outputs": [],
      "source": [
        "#Calculating the R2 value\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R2 :\" , r2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the Adjusted R2 value\n",
        "adjusted_R2=1-(1-r2)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2:\" , adjusted_R2)"
      ],
      "metadata": {
        "id": "5WAWISA_hf6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculatiing the Mean Squared Error\n",
        "MSE=mean_squared_error(y_test,y_pred)\n",
        "print(\"MSE:\" , MSE)"
      ],
      "metadata": {
        "id": "I7s4B4y-pry_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the Root Mean Squared Error.\n",
        "RMSE=np.sqrt(MSE)\n",
        "print(\"RMSE:\" , RMSE)"
      ],
      "metadata": {
        "id": "aiQ4n7a8syyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Linear regression algorithm to create the model. \n",
        "I did get good results for this model.\n",
        "I have got the 'reg_score' as 1 whis is a pretty good value.\n",
        "Also the 'Coeffient of determination or the R2 ' value is also 1. R2\n",
        " is defined as :\n",
        "            (1-u/v), where \n",
        " 'u' is the residual sum of squares ((y_true - y_pred)** 2).sum() and \n",
        " 'v' is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
        "Coefficient of determination also called as R2 score is used to evaluate the performance of a linear regression model. It is the amount of the variation in the output dependent attribute which is predictable from the input independent variable(s) \n",
        "I have got 'Adjusted R2' value as 1.\n",
        "Adjusted R2 also indicates how well terms fit a curve or line, but adjusts for the number of terms in a model. If we add more and more useless variables to a model, adjusted r-squared will decrease. If we add more useful variables, adjusted r-squared will increase.It is defined as:\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  R2(adjusted)=1-[(1-R2)(n-1)/n-k-1], where\n",
        "\n",
        "  \n",
        "  'n' is the number of datapoints in data sample\n",
        "  'k' is the number of independent variables.\n",
        "\n",
        "\n",
        "The MSE value is found to be:5.3217e-28 and the RMSE value is foumd to be:2.3068e-14\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "221W-59JMa_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "wTe8K5rdYGV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "yyzM232tatvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "OMC09DFCbrEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - **Implementing Ridge and Lasso Regression:**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation Of Ridge Regression\n",
        "# Create an instance of the Ridge Regressor\n",
        "L2=Ridge(alpha=0.1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "L2.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "ULgBUAmTpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "y_pred_ridge= L2.predict(X_test)\n",
        "y_pred_ridge_train=L2.predict(X_train)"
      ],
      "metadata": {
        "id": "obC9hzz3Q4Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score=L2.score(X_train,y_train)\n",
        "print(\"Ridge Score:\" ,score )"
      ],
      "metadata": {
        "id": "ttTLxOcxdERX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_ridge=pd.DataFrame(zip(y_test, y_pred_ridge), columns = ['actual', 'pred'])\n"
      ],
      "metadata": {
        "id": "W0x_tjm_5VkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_ridge.sample(20).plot(kind='bar',figsize=(14,8))\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n"
      ],
      "metadata": {
        "id": "hzUsy8N171qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plot is a bar plot that shows the actual and compared vales for each of the observations.Bar charts should be used when you are showing segments of information.Vertical bar charts are useful to compare different categorical or discrete variables, such as age groups, classes etc."
      ],
      "metadata": {
        "id": "CLIIlvb5-qWT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUxepkUaQ41y"
      },
      "source": [
        "#Implementing the Lasso algorithm\n",
        "#Creating a Lasso Model\n",
        "L1 = Lasso(alpha = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XP_cH9MQ41z"
      },
      "source": [
        "#fitting the model\n",
        "L1.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv3GUdjcQ41z"
      },
      "source": [
        "#Making predictions for  the train and test data\n",
        "y_pred_lasso = L1.predict(X_test)\n",
        "y_pred_lasso_train=L1.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ov_RzbvQ41z"
      },
      "source": [
        "lasso_score=L1.score(X_train, y_train)\n",
        "print(\"Score for Lasso regression:\" , lasso_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apyoiceXQ410"
      },
      "source": [
        "predictions_lasso=pd.DataFrame(zip(y_test, y_pred_lasso), columns = ['actual', 'pred'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_lasso.sample(20).plot(kind=\"bar\" , figsize=(14,8))\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')"
      ],
      "metadata": {
        "id": "u2S3UOZT8lAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plot is a bar plot that shows the actual and compared vales for each of the observations.Bar charts should be used when you are showing segments of information.Vertical bar charts are useful to compare different categorical or discrete variables, such as age groups, classes etc."
      ],
      "metadata": {
        "id": "XJt9MgDs9o18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the R2 value for the Ridge regression algorithm\n",
        "r2_ridge_train=r2_score(10**(y_train) ,10**(y_pred_ridge_train))\n",
        "print(\"R2 for train data set :\" , r2_ridge_train)\n",
        "r2_ridge=r2_score(10**(y_test) ,10**(y_pred_ridge))\n",
        "print(\"R2 for test data set:\" , r2_ridge)"
      ],
      "metadata": {
        "id": "80Jo2OkPA7f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the MSE and RMSE for Ridge\n",
        "MSE  = mean_squared_error(10**(y_test),10**(y_pred_ridge))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)"
      ],
      "metadata": {
        "id": "dR2Tch_lDHRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating Adjusted R2 for Ridge\n",
        "adjusted_R2=1-(1-r2_ridge)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 for Ridge:\" , adjusted_R2)"
      ],
      "metadata": {
        "id": "Q521ETUTCH4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the R2 value for the Lasso regression algorithm\n",
        "r2_lasso_train=r2_score(y_train , y_pred_lasso_train)\n",
        "print(\"R2 for train data set :\" , r2_lasso_train)\n",
        "r2_lasso=r2_score(y_test , y_pred_lasso)\n",
        "print(\"R2 for test data set:\" , r2_lasso)"
      ],
      "metadata": {
        "id": "6ZBxPYMaEJmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the MSE and RMSE for Lasso\n",
        "MSE  = mean_squared_error(10**(y_test),10** (y_pred_lasso))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)"
      ],
      "metadata": {
        "id": "74WbFXcOFoFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating Adjusted R2 for Lasso\n",
        "adjusted_R2_lasso=1-(1-r2_lasso)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 for Lasso:\" , adjusted_R2_lasso)"
      ],
      "metadata": {
        "id": "se2aTuXXFoFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge and lasso regression are two common machine learning approaches for constraining model parameters. Both methods try to get the coefficient estimates as close to zero as possible because minimizing (or shrinking) coefficients can reduce variance dramatically (i.e., overfitting)\n",
        "\n",
        "For Ridge algorithm I have got :\n",
        "\n",
        "R2 as 0.999 for train data.For test data also the R2 value is found to be 0.99\n",
        "\n",
        "Adjusted R2 for test data is found to be:0.999\n",
        "\n",
        "MSE (Mean Squared Error) is found to be:105.321\n",
        "\n",
        "RMSE(Root Mean Squared Error) is found to be:10.26\n",
        "\n",
        "For Lasso regression:\n",
        "\n",
        "For Lasso regression the R2 value is found to be the same for the train data and test data , which is 0.777.\n",
        "\n",
        "The Adjusted R2 value is found to be also 0.777\n",
        "\n",
        "The MSE value is found to be:29341384318141.273\n",
        "\n",
        "The Rmse value is found to be:5416768.807891035\n",
        "\n"
      ],
      "metadata": {
        "id": "DRAoUYJ5VXr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "#Cross validation and yperparameter tuning for Ridge regression.\n",
        "ridge_cv=Ridge()\n",
        "parameter= {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "ridge_regressor = GridSearchCV(ridge_cv, param_grid=parameter, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "3A9aAwIVQsd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "-7jjYJiTT_x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuFUkUFtzOYu"
      },
      "source": [
        "y_pred_ridge_cv = ridge_regressor.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBiPj1GWzSz1"
      },
      "source": [
        "MSE  = mean_squared_error(10**(y_test), 10**(y_pred_ridge_cv))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test), 10**(y_pred_ridge_cv))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(10**(y_test), 10**(y_pred_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross validation for Lasso algorithm\n",
        "lasso_cv= Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014]}\n",
        "lasso_regressor = GridSearchCV(lasso_cv, param_grid=parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "kzOQQhK2byEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "AGDz1qnlb_ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lasso_cv = lasso_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "6geN-mQ5cWdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE  = mean_squared_error(10**(y_test), 10**(y_pred_lasso_cv))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test), 10**(y_pred_lasso_cv))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(10**(y_test), 10**(y_pred_lasso_cv)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "euW4apqvdddx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our main aim is  to discover the best hyperparameters values to get the perfect prediction results from our model.  But the challenge is, how to find these best sets of hyperparameters? If we try the Manual Search method,by hit and trial process,It would take huge amount of time to build one single model and may also require huge human resource.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced.\n",
        "Grid searching is a method to find the best possible combination of hyper-parameters at which the model achieves the highest accuracy. Before applying Grid Search on any algorithm, data is divided into train and test data set . GridSearchCV  uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance. \n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model. \n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Ridge algorithm , the MSE has become  7.65 ,the RMSE value has become \n",
        "8.75.The R2 value  has reached 100% and Adjusted R2 value has reached 100%.Before applying the Grid Search Cv technique the RMSE and MSE values were very high and now it has drastically reduced and also achieving 100%R2 qnd AdjustedR2.\n",
        "\n",
        "For Lasso algorithm ,  \n",
        "The  MSE value has become:  and RMSE value has become :\n",
        "Compared to the model before applying hyperparameter technique , the MSE and RMSE value has drastically reduced.\n",
        "The R2 value has reached 100% and Adjusted R2 value has reached 100%.\n",
        "Before applying the hyperparameter optimization technique , the R2 and Adjusted R2 value was 77%.\n",
        "Hence, by usding the hyper parameter optimization , we have seen much improvement in the model."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML MODEL -3- :Implementing Random Forest Regressor"
      ],
      "metadata": {
        "id": "KjWx-zdqsH8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instance of Random Forest\n",
        "random_forest=RandomForestRegressor()\n"
      ],
      "metadata": {
        "id": "fBhXpsN7sHgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the model\n",
        "random_forest.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "Kr7Rza1xsFT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the model\n",
        "y_pred_rf=random_forest.predict(X_test)"
      ],
      "metadata": {
        "id": "6kLiU_L7tg_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSbsPYtyLhrg"
      },
      "source": [
        "predictions_rf=pd.DataFrame(zip(y_test, y_pred_rf), columns = ['actual', 'pred'])\n",
        "predictions_rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_rf.sample(20).plot(kind=\"bar\" , figsize=(14,8))\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')"
      ],
      "metadata": {
        "id": "kfjVDjNILhrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plot is a bar plot that shows the actual and compared values for each of the observations.Bar charts should be used when we are showing segments of information.Vertical bar charts are useful to compare different categorical or discrete variables, such as age groups, classes etc."
      ],
      "metadata": {
        "id": "Jx-aoltrMdM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "RSEvDokvGlJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the MSE\n",
        "rf_MSE=mean_squared_error(10**(y_test) ,10**( y_pred_rf))\n",
        "print(\"MSE of Random Forest model :\" , rf_MSE)"
      ],
      "metadata": {
        "id": "oxIm8QUFtuaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the RMSE\n",
        "rf_RMSE=np.sqrt(rf_MSE)\n",
        "print(\"RMSE of Random Forest Model :\" , rf_RMSE)"
      ],
      "metadata": {
        "id": "29CUXOoKuNwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the R2 value:\n",
        "RF_r2=r2_score(y_test , y_pred_rf)\n",
        "print(\"R2 value for Random Forest Model:\" , RF_r2 )"
      ],
      "metadata": {
        "id": "hB3AwSOIuhPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the Adjusted R2\n",
        "adjusted_R2=1-[(1-RF_r2)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))]\n",
        "print(\"Adjusted R2:\" , adjusted_R2)\n"
      ],
      "metadata": {
        "id": "37Uayjvans8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I have used the Random Forest algorithm to create the model and I also good result in this. Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.I also got good result in this algorithm.\n",
        "\n",
        "I have got the MSE as:4.163688743600329e-05 which is pretty less.\n",
        "Also I have got RMSE value as:almost zero.\n",
        "\n",
        "Also the R2 value is found to be 0.999"
      ],
      "metadata": {
        "id": "skJTRtcYGKtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "XZHkJG4ELAJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyper parameter tuning of Random Forest regressor.\n",
        "# Number of trees\n",
        "n_estimators = [80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]"
      ],
      "metadata": {
        "id": "l22paPIywWLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "KS-bPwqcOy9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the RandomForestRegressor\n",
        "rf_model_cv = RandomForestRegressor()\n",
        "\n",
        "# Grid search\n",
        "rf_gridsearch = GridSearchCV(estimator=rf_model_cv,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 2,verbose=2 ,  scoring='neg_mean_squared_error')\n"
      ],
      "metadata": {
        "id": "SNJfsISEPByL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_gridsearch.fit(X_train , y_train)"
      ],
      "metadata": {
        "id": "UbIYH5DVQLjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_gridsearch.best_estimator_"
      ],
      "metadata": {
        "id": "5QOkiAxmn8ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_optimal_model = rf_gridsearch.best_estimator_"
      ],
      "metadata": {
        "id": "O0OWFSGwotX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_gridsearch.best_params_"
      ],
      "metadata": {
        "id": "Rys2Pbvoo3lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data. \n",
        "ytrain_rfgrid=rf_optimal_model.predict(X_train)\n",
        "ytest_rfgrid=rf_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "_lcqLcjcpPhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the MSE value for test data\n",
        "rfgrid_MSE=mean_squared_error(y_test , ytest_rfgrid)\n",
        "print(\"MSE value:\" , rfgrid_MSE)"
      ],
      "metadata": {
        "id": "WJSAHj4aqG8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the RMSE value\n",
        "rfgrid_RMSE=np.sqrt(rfgrid_MSE)\n",
        "print(\"RMSE value:\" , rfgrid_RMSE)"
      ],
      "metadata": {
        "id": "rBHS3DOEqxFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the R2 value:\n",
        "RFgrid_r2=r2_score(y_test , ytest_rfgrid)\n",
        "print(\"R2 value:\" , RFgrid_r2 )"
      ],
      "metadata": {
        "id": "yGIIdbOArO0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the Adjusted R2\n",
        "adjusted_R2rfgrid=1-(1-RFgrid_r2)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2:\" , adjusted_R2xgbgrid)\n"
      ],
      "metadata": {
        "id": "xQ2-lJWtTLTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?\n"
      ],
      "metadata": {
        "id": "OOImPXtwTLTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our main aim is  to discover the best hyperparameters values to get the perfect prediction results from our model.  But the challenge is, how to find these best sets of hyperparameters? If we try the Manual Search method,by hit and trial process,It would take huge amount of time to build one single model and may also require huge human resource.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced.\n",
        "Grid searching is a method to find the best possible combination of hyper-parameters at which the model achieves the highest accuracy. Before applying Grid Search on any algorithm, data is divided into train and test data set . GridSearchCV  uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance. \n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model. \n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "64Svl-DBUKL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart**."
      ],
      "metadata": {
        "id": "d5257TGETLTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MDNPDKjkhpeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - **Implementing XgBoost Regressor**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 - Implementation of XGBoost Regressor\n",
        "# Create an instance of the XGBoost Regressor\n",
        "xgb_model = XGBRegressor()\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgb_model=xgb_model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "ytrain_xgb_pred = xgb_model.predict(X_train)\n",
        "ytest_xgb_pred = xgb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_xgb=pd.DataFrame(zip(y_test , ytest_xgb_pred) , columns=['actual' , 'predicted'] )\n",
        "predictions_xgb"
      ],
      "metadata": {
        "id": "yiGR3I7Lg1EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_xgb.sample(20).plot(kind=\"bar\" , figsize=(14,8))\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')"
      ],
      "metadata": {
        "id": "eazgvIZUhqKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLMHFSXSy48N"
      },
      "source": [
        "#Calculating the MSE\n",
        "xgb_MSE=mean_squared_error(10**(y_test) ,10**( ytest_xgb_pred))\n",
        "print(\"MSE of Random Forest model :\" , xgb_MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the RMSE\n",
        "xgb_RMSE=np.sqrt(xgb_MSE)\n",
        "print(\"RMSE of XGB model :\" , xgb_RMSE)"
      ],
      "metadata": {
        "id": "YDZwdmNqy48P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the R2\n",
        "xgb_R2=r2_score(y_test , ytest_xgb_pred)\n",
        "print(\"R2 for the XGB  model:\" , xgb_R2)"
      ],
      "metadata": {
        "id": "JAlBgPjdy48O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4yEhj5ay48P"
      },
      "source": [
        "#Calculating the Adjusted R2\n",
        "adjusted_R2=1-(1-xgb_R2)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2:\" , adjusted_R2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, I used XGBoost algorithm to create the model. As I got there  good result.\n",
        "\n",
        "For training dataset, i found precision of 100% and recall of 91% and f1-score of 95% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 46% and recall of 95% and f1-score of 62%. Accuracy is 92% and average percision, recall & f1_score are 73%, 93% and 79% respectively with a roc auc score of 72%.\n",
        "\n",
        "For testing dataset, i found precision of 99% and recall of 90% and f1-score of 94% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 35% and recall of 80% and f1-score of 48%. Accuracy is 90% and average percision, recall & f1_score are 67%, 85% and 71% respectively with a roc auc score of 66%.\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique."
      ],
      "metadata": {
        "id": "ctnr9FepURk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV)\n",
        "# Number of trees\n",
        "n_estimators = [80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "xgb_model_cv = XGBRegressor()\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Grid search\n",
        "xgb_grid = GridSearchCV(estimator=xgb_model_cv,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 3 , scoring='neg_mean_squared_error')\n",
        "\n",
        "xgb_grid=xgb_grid.fit(X_train,y_train)\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "#ytrainxgb_preds = xgb_grid.predict(X_train)\n",
        "#ytestxgb_preds = xgb_grid.predict(X_test)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best: %f using %s\" % (xgb_grid.best_score_, xgb_grid.best_params_))"
      ],
      "metadata": {
        "id": "70UCwE6KzskE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_optimalmodel=xgb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "nQF-EiRb2ksZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "ytrain_xgbgrid_pred = xgb_optimalmodel.predict(X_train)\n",
        "ytest_xgbgrid_pred = xgb_optimalmodel.predict(X_test)"
      ],
      "metadata": {
        "id": "TzKMHqej22i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrozapM9SlKu"
      },
      "source": [
        "#Calculating the MSE\n",
        "xgbgrid_MSE=mean_squared_error(10**(y_test) ,10**( ytest_xgbgrid_pred))\n",
        "print(\"MSE of XGB grid  model :\" , xgbgrid_MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the RMSE\n",
        "xgbgrid_RMSE=np.sqrt(xgbgrid_MSE)\n",
        "print(\"RMSE of XGB model :\" , xgbgrid_RMSE)"
      ],
      "metadata": {
        "id": "YTPJDXv4SlKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the R2 value:\n",
        "r2_xgbgrid=r2_score(y_test , ytest_xgbgrid_pred)\n",
        "print(\"R2 value:\" , r2_xgbgrid )"
      ],
      "metadata": {
        "id": "mJVt9VV0Ptxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the Adjusted R2\n",
        "adjusted_R2xgbgrid=1-(1-r2_xgbgrid)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2:\" , adjusted_R2xgbgrid)\n"
      ],
      "metadata": {
        "id": "uv0FrW5IPtxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our main aim is  to discover the best hyperparameters values to get the perfect prediction results from our model.  But the challenge is, how to find these best sets of hyperparameters? If we try the Manual Search method,by hit and trial process,It would take huge amount of time to build one single model and may also require huge human resource.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced.\n",
        "Grid searching is a method to find the best possible combination of hyper-parameters at which the model achieves the highest accuracy. Before applying Grid Search on any algorithm, data is divided into train and test data set . GridSearchCV  uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance. \n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model. \n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "1QqICdO_rCol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would like to go with R2 and Adjusted R2.\n",
        "\n",
        "R Square measures how much variability in dependent variable can be explained by the model.R Square is calculated by the sum of squared of prediction error divided by the total sum of the square which replaces the calculated prediction with mean. R Square value is between 0 to 1 and a bigger value indicates a better fit between prediction and actual value.\n",
        "\n",
        "R Square is a good measure to determine how well the model fits the dependent variables. However, it does not take into consideration of overfitting problem. If your regression model has many independent variables, because the model is too complicated, it may fit very well to the training data but performs badly for testing data. That is why Adjusted R Square is introduced because it will penalize additional independent variables added to the model and adjust the metric to prevent overfitting issues.Hence I would like to give more importance to Adjusted R2 than R2 "
      ],
      "metadata": {
        "id": "3EmhLzHRsg5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above snap shot, we can clearly see that for XGBoost has a improved score of f1_score over random forest. the accuracy and roc auc score is also improved for xg boost. The recall might be high in random forest but when it comes to precision and f1_score for True Churn Data scores, it's very low and even in averge scores of precision and f1_score is too low. In case of xgboost it's higher than that of random forest. \n",
        "\n",
        "So, I have chosen XGBoost as the final prdiction model which should be deployed for real user interaction."
      ],
      "metadata": {
        "id": "h0DZUNlUa1-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize JavaScript visualizations in notebook environment\n",
        "shap.initjs()\n",
        "# Define a tree explainer for the built model\n",
        "explainer = shap.TreeExplainer(xgb_optimalmodel)\n",
        "# obtain shap values for the first row of the test data\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[0])"
      ],
      "metadata": {
        "id": "VqJ1s7mnWEKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.dtype"
      ],
      "metadata": {
        "id": "WKLobR0o34MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get shap values\n",
        "#data = shap.sample(X_train, 100)\n",
        "feature_names = list(nyc_data.columns)\n"
      ],
      "metadata": {
        "id": "HwMWtgHMcp2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_data"
      ],
      "metadata": {
        "id": "4ei-rWkj4YFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values = shap.TreeExplainer(xgb_optimalmodel).shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test , feature_names=feature_names , plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "gdOhz5lZWJ2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "nfT8VV_qZ43J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Waterfall plot for first observation\n",
        "shap.plots.waterfall(shap_values[0])"
      ],
      "metadata": {
        "id": "xsJ6cvtwdT6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand this, lets dive into our first SHAP plot. Above we have the code to create a waterfall plot for the first abalone in our dataset. This plot helps us visualise the SHAP values for each of the features. These tell us how much each of the features have increased or decreased the predicted number of rings for this specific abalone.\n",
        "\n",
        "Looking at the x-axis, we can see the base value is E[f(x)] = -1.94. This is the average predicted number of rings across all 2967 abalones. The ending value is f(x) = -0.396. This is the predicted churn type for this abalone. The SHAP values are all the values in between. For example, the international plan  increased the predicted class of churn type by 1.03 when compared to the Account Length.\n",
        "\n",
        "There will be a unique waterfall plot for every observation/abalone in our dataset. They can all be interpreted in the same way as above. In each case, the SHAP values tell us how much each factor contributed to the models prediction when compared to the mean prediction. Large positive/negative SHAP values indicate that the feature had a significant impact on the models prediction."
      ],
      "metadata": {
        "id": "kQZX0I3YeiQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize JavaScript visualizations in notebook environment\n",
        "shap.initjs()\n",
        "# Forceplot for first observation\n",
        "shap.plots.force(shap_values[0])"
      ],
      "metadata": {
        "id": "OEAwgQY5d6wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to visualise SHAP values is using a force plot. These give us pretty much the same information as a waterfall plot in a straight line how the geatures are contributiong and the relationship between the features. If one value value increases res will compress. you can see we start at the same base value of -1.94."
      ],
      "metadata": {
        "id": "a3J1xsxxgKO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get expected value and shap values array\n",
        "expected_value = explainer.expected_value\n",
        "shap_array = explainer.shap_values(X_test)\n",
        "\n",
        "#Descion plot for first 10 observations\n",
        "shap.decision_plot(expected_value, shap_array[0:10],feature_names=list(X_test.columns))"
      ],
      "metadata": {
        "id": "2WPLpsukeKMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Waterfall and force plots are great for interpreting individual predictions. To understand how our model makes predictions in general we need to aggregate the SHAP values. One way to do this is using a decision plot. Above we have the code used to output the decision plot for the first 10 abalones.\n",
        "\n",
        "we can see there are 10 lines in the plot one for each abalone. They all start at the same base value of -1.94 and end at their final predicted number of rings. As you move up from each feature on the y-axis, the movement on the x-axis is given by the SHAP value for that feature. Ultimately, this gives you similar information to a waterfall plot except we can now see it for multiple observations.\n",
        "\n",
        "With only 10 observations, we can already see some trends. For example, some of the lines seem to zig-zag at the top of the chart. For these observations, the shucked weight increases the prediction (i.e. positive SHAP) and the shell weight and whole weight decrease the prediction (i.e. negative SHAP). In other words, these features have opposite effects on the prediction. When we look at plot 5  beeswarm, we will see that this is the case for the model in general.\n",
        "\n",
        "An issue with this chart is that we cant use it to visualise all the SHAP values at once. As you increase the number of observations it will become too cluttered and it will be difficult to gain any understanding of how the model works. We are limited to interpreting the SHAP values for a handful of observations."
      ],
      "metadata": {
        "id": "ZlCtCsFvgtxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean SHAP\n",
        "shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "dlPe8NFNePY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to aggregate the values is using a mean SHAP plot. For each feature, we calculate the mean of the absolute SHAP values across all observations. We take the absolute values as we do not want positive and negative values to offset each other. In the end, we have the bar plot above. There is one bar for each feature and we can see that shell weight had the largest mean SHAP out of all the features.\n",
        "\n",
        "Features that have large mean SHAP values will tend to have large positive/negative SHAP values. In other words, these are the features that have a significant impact on the models predictions. In this sense, this plot can be used in the same way as a feature importance plot. That is to highlight features that are important to a models predictions. An issue is that it does not tell us anything about the nature of the relationship between features and the target variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tno74FAchBGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P)_# Beeswarm plot\n",
        "shap.plots.beeswarm(shap_values)"
      ],
      "metadata": {
        "id": "qVYvFUBdeV2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our final aggregation is the beeswarm plot. As seen in Figure, this is a plot of all the SHAP values. The values are grouped by the features on the y-axis. For each group, the colour of the points is determined by the value of the same feature (i.e. higher feature values are redder). The features are ordered by the mean SHAP values.\n",
        "\n",
        "This plot addresses some of the issues in the previous plots. Unlike the decision plot, we can plot all the observations and still have clear interpretations. Like mean SHAP, it can be used to highlight important relationships. We can also start to understand the nature of these relationships. For example, for Day_1call_duration notice how as the feature value increases the SHAP values increase.\n"
      ],
      "metadata": {
        "id": "cWCeVLZghnB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing Blog**\n"
      ],
      "metadata": {
        "id": "eLTSyMe3_MXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/almabetter/data-preprocessing-ea09fac6a7f7"
      ],
      "metadata": {
        "id": "XYNmUe2H_pPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)"
      ],
      "metadata": {
        "id": "BpAj0Mhk-kdr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evtxQtoczOK4"
      },
      "source": [
        "importances = xg_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJoQ_0sUzOK5"
      },
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaxEFDeUzOK5"
      },
      "source": [
        "features = X_train.columns\n",
        "importances = xg_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvYA_tJ3zOK6"
      },
      "source": [
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dgKJbOhaAvZB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VYJo-3cd3Fi"
      },
      "source": [
        "importances = rf_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbG5FRy7d3Fk"
      },
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ft22nvPd3Fl"
      },
      "source": [
        "features = X_train.columns\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5nfbZ0td3Fl"
      },
      "source": [
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KBpQmVbazgHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Grid search\n",
        "rf_grid = GridSearchCV(estimator=rf_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='f1')\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_grid.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "train_class_preds = rf_grid.predict(X_train)\n",
        "test_class_preds = rf_grid.predict(X_test)\n"
      ],
      "metadata": {
        "id": "PTp0foAWzgir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best: %f using %s\" % (rf_grid.best_score_, rf_grid.best_params_))"
      ],
      "metadata": {
        "id": "sFxlFLh8zgit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlqW_ER6zgiu"
      },
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "T-6y2LGHzgiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))"
      ],
      "metadata": {
        "id": "ELt7j5whzgiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah2bVViyzgiv"
      },
      "source": [
        "# Hypertuned Random Forest\n",
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZk5xnCIP3UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHazRuY_0RRK"
      },
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_test, test_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "t09GuUml0RRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(train_class_preds, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_class_preds))"
      ],
      "metadata": {
        "id": "jbfBpu_y0RRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hglw0pWL0RRL"
      },
      "source": [
        "print(metrics.classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, test_class_preds))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}